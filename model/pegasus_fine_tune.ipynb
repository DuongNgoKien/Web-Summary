{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db22c49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DuongNgoKien\\AppData\\Local\\anaconda3\\envs\\api\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer, Trainer, TrainingArguments\n",
    "import evaluate\n",
    "import torch\n",
    "import evaluate\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cb145c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "output_dir = './results'\n",
    "model_name = 'google/pegasus-large'\n",
    "text_path = 'texts.txt'\n",
    "label_path = 'labels.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1774a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.encoder.embed_positions.weight', 'model.decoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30ea893a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading config.json: 100%|██████████| 1.39k/1.39k [00:00<00:00, 20.2kB/s]\n",
      "C:\\Users\\DuongNgoKien\\AppData\\Local\\anaconda3\\envs\\api\\lib\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\DuongNgoKien\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading pytorch_model.bin: 100%|██████████| 2.28G/2.28G [03:42<00:00, 10.2MB/s]\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.encoder.embed_positions.weight', 'model.decoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Downloading generation_config.json: 100%|██████████| 259/259 [00:00<?, ?B/s] \n"
     ]
    }
   ],
   "source": [
    "model = PegasusForConditionalGeneration.from_pretrained('google/pegasus-xsum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4703362a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading tokenizer_config.json: 100%|██████████| 87.0/87.0 [00:00<?, ?B/s]\n",
      "Downloading spiece.model: 100%|██████████| 1.91M/1.91M [00:00<00:00, 5.57MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 65.0/65.0 [00:00<00:00, 8.17kB/s]\n",
      "Downloading tokenizer.json: 100%|██████████| 3.52M/3.52M [00:00<00:00, 6.13MB/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = PegasusTokenizer.from_pretrained('google/pegasus-xsum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "195dec3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.encoder.embed_positions.weight', 'model.decoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/pegasus-xsum\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/pegasus-xsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5034fa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('xsum')\n",
    "train_texts, train_labels = dataset['train']['document'][:1000], dataset['train']['summary'][:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4feb4c50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Plans to give English MPs a veto over laws affecting England would be \"irresponsible\", Ed Miliband has warned.']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(train_texts[200], max_length=512, truncation=True, return_tensors='pt')\n",
    "summary_ids = model.generate(inputs[\"input_ids\"], num_beams=4, max_length=50)\n",
    "tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8ad425e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(train_texts[200], max_length=1024, truncation=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "22cec86d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c163c629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In a Commons debate, the ex-Labour leader said Conservative plans to limit the voting powers of Scottish MPs on Commons laws would \"rip up\" hundreds of years of parliamentary procedure.\\nThe SNP said the Conservatives wanted to create a \"quasi-English Parliament\".\\nBut ministers said it was vital England was treated fairly as further powers were devolved to other parts of the UK.\\nAt the end of the debate, Labour staged and won a vote in which the government abstained. And Conservative MP David Davis raised a point of order to urge the government to allow more time for the matter to be considered.\\nThe government believes bills applying exclusively to England should not become law without the explicit consent of MPs from English constituencies and it wants to change Commons rules known as standing orders to give them a \"decisive say\" during their passage.\\nMinisters say this will address the longstanding anomaly by which Scottish MPs can vote on issues such as health and education affecting England but English MPs have no say on similar matters relating to Scotland, where such policies are devolved.\\nMark D\\'Arcy, BBC Parliamentary correspondent\\nPanic stations? From the point of view of the government whips this afternoon\\'s emergency Commons debate on English Votes for English Laws was really rather alarming.\\nThe debate, so skilfully secured by Lib Dem ex Scottish secretary Alistair Carmichael, produced a slightly pointless vote in which the Conservatives mostly abstained, but it brought all kinds of nasty tensions to the surface.\\nConservative MPs are supposed to be signed up for EVEL under the terms of their manifesto, but there were clearly quite a few with doubts, some about the policy, many more about the process, which was Mr Carmichael\\'s line of attack.\\nRead the article in full\\nHowever, the plans came under sustained attack from Labour, SNP and Lib Dems in an urgent debate, secured by former Scottish Secretary Alistair Carmichael.\\nMPs are due to debate and vote on the government\\'s plans next week but Mr Carmichael, the MP for Orkney and Shetland, said the plans required more detailed consideration, arguably through primary legislation.\\nIf MPs representing constituencies in England had a \"veto\" on certain legislation, it would breach the longstanding principle that MPs, no matter who they represented, were \"all equal\".\\n\"To go as far as the government wants to go in the timescale they want to go brings with it an attendant level of risk that I would consider to be irresponsible,\" he said.\\n\"They are not entitled to use the UK Parliament as a proxy for an English Parliament.\"\\nAssurances that the new system would be reviewed by MPs after a year were inadequate, he added, saying this would not be capable of \"putting a dangerous genie back in the bottle after it had been let out - we all know that is the political reality\".\\nBut Commons Leader Chris Grayling said it was \"simply incorrect\" to claim some MPs would be prevented from debating and voting on certain legislation and would continue to exercise the same rights as they do now.\\nPressed by Mr Miliband on \"what the meaning and definition\" of English-only legislation would be, Mr Grayling said it would be up to the Speaker to decide but he believed a \"simple test\" would be what things were devolved to Scotland.\\nRequiring bills to have the support of a \"double majority\" of the whole of the Commons and those MPs representing England would also help mitigate \"any resentment\" felt by English voters about the slower pace of devolution to England.\\n\"It is of vital importance that English citizens of the UK, as we move to an extra layer of devolution to Scotland and Wales and devolve additional tax powers to Northern Ireland, that they think it is fair,\" he said.\\n\"It is what we pledged to do in our manifesto. We set it out in detail, step by step by step. We are implementing these changes and keeping our promises. I think the people who elected us would expect nothing else.\"\\nMr Miliband said the Conservatives had the power to address the issue after winning the election but urged them to think again.\\n\"Is this true to the traditions of Conservatism? No because the last thing you do is rip up hundreds of years of constitutional practice in a standing order vote just before the House goes into recess,\" he said.\\n\"Doing this procedure in the way it is being proposed is an act of constitutional vandalism. It really is.\"\\nThe SNP said Scottish voters would be affected by legislation on schools and NHS budgets in England through the Barnett Formula used to allocate public spending to different nations of the UK.\\n\"This not just English votes for English laws, this is English votes for Scottish laws,\" said Pete Wishart, MP for Perth and North Perthshire. \"It is totally and utterly unacceptable.\"\\nHe added: \"Why don\\'t they just tattoo our foreheads \\'Scottish\\' and then they would be able to identify us.\"\\nAnd Conservative MP Sir Edward Leigh said the move would not make any difference to the outcome of votes and urged ministers to omit laws which indirectly applied to Scotland, saying not to do so would be a \"gift\" to the SNP\\'s independence campaign.\\nResponding to a question by an SNP MP on Monday, Mr Grayling said the only English-only measure during the last Parliament had been the Education Bill, and there were 13 Bills which applied to England and Wales.'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2ea69786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15458,  2893,   115,  ...,   111,   109,     1]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bced5201",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2, 79672,   131,   116,  1258,   120,  3341,  3308,   112,  5289,\n",
       "           685,  2101,   111,  1352,  2910,   190,   153,  2382,   107, 25688,\n",
       "           195,  5366,   134,  9009,   155,  2127,   134, 30376,   107,  6601,\n",
       "           108, 46422,   109,   301,   134,   154,   197,  5886, 10970,   107,\n",
       "          3096,   680,   117,   263,   141,   524,   330,  2207, 17267,   108,\n",
       "         15845,   111, 11143,   108,   162,  1481,   109,   552,   112, 34605,\n",
       "           109,  1916,   944,  1586,   113,  2456,   173,   157,   443,  4828,\n",
       "           107,   139,  9200,   118, 79672,  2853,   148, 26518,  2714,   131,\n",
       "          3840,   115,   109,   787,   552,  1827,   107, 38737,   697,   120,\n",
       "           203,  1147,  4042,   407,   256,   129,   114,  1259,  1243,   118,\n",
       "           176,   198, 19122, 19182,   116,   194,   383,   112,   275,   481,\n",
       "           107,  3641,   202, 47731,   108,  3522,  1237,  3319,   134, 71817,\n",
       "          4393,  2039,   108,   243,   151, 39902,   287,   232,  1100,   148,\n",
       "           288,   174,   142, 39679,  6620,   115,   177, 23818,   111,  4714,\n",
       "          1256,   125,   131,   208,  3825,   120,   109,   924,   264,   138,\n",
       "         23825,   176, 21937,   116,  1126, 64953,   481,  5942,  1100,   496,\n",
       "             1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "06ef70f3",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<mask_1> Twilio\\'s platform that enables developers to incorporate phone calls and text messages into their apps. Shares were priced at $15 but closed at $28.19, valuing the company at more than $2bn. Its software is used by companies including OpenTable, WhatsApp and Uber, which uses the technology to withhold the actual mobile numbers of drivers when they call passengers. The appetite for Twilio shares has revived investors\\' hopes in the US technology sector. Analysts believe that its successful debut market could be a positive sign for other \"unicorns\" looking to go public. Jack Ablin, chief investment officer at BMO Private Bank, said: \"[This year] has really been an arid desert in new issuance and equity ... I\\'m encouraged that the success here will spawn other IPOs [initial public offerings].\"'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts[502]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e528e591",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[139, 357, 519, 113, 1303, 115, 12455, 9246, 108, 156, 113, 109, 633, 3741, 2790, 108, 117, 309, 270, 9068, 107, 4308, 201, 117, 3121, 115, 6334, 12358, 111, 223, 4194, 115, 91586, 14626, 1686, 8867, 2790, 141, 2716, 336, 107, 41558, 124, 109, 3381, 3682, 54521, 749, 12071, 640, 112, 1303, 134, 109, 1946, 59340, 15258, 22947, 107, 2, 1485, 2965, 23164, 59614, 3333, 109, 345, 112, 11028, 109, 1303, 107, 139, 6500, 31670, 114, 11210, 1075, 108, 10233, 223, 1162, 1746, 124, 5156, 1411, 233, 109, 674, 1553, 55976, 107, 62273, 20678, 108, 170, 11216, 109, 30220, 9372, 162, 140, 8867, 2790, 108, 243, 265, 256, 146, 5709, 109, 1546, 121, 44224, 1407, 559, 109, 6172, 1194, 107, 611, 108, 265, 243, 154, 16530, 201, 256, 133, 174, 2777, 165, 112, 615, 109, 11210, 1075, 368, 146, 3656, 107, 198, 362, 117, 1011, 155, 125, 171, 311, 186, 117, 167, 249, 14447, 118, 65340, 111, 109, 8152, 307, 233, 111, 125, 2402, 2504, 120, 233, 155, 126, 117, 744, 172, 145, 131, 216, 13954, 132, 6460, 745, 265, 243, 107, 198, 1435, 218, 146, 129, 921, 155, 126, 117, 1850, 161, 2708, 204, 109, 289, 324, 390, 107, 198, 2393, 195, 119, 146, 734, 112, 225, 214, 114, 588, 154, 173, 109, 4752, 111, 109, 5929, 9737, 196, 1871, 165, 3801, 8199, 108, 114, 6172, 6034, 2085, 115, 295, 482, 109, 24417, 262, 113, 109, 3357, 2750, 107, 91586, 140, 8867, 1194, 141, 743, 108, 43011, 2101, 112, 4094, 154, 55244, 115, 109, 345, 107, 2, 139, 9083, 2905, 131, 116, 11501, 6603, 1919, 5296, 61011, 140, 115, 6334, 12358, 124, 1491, 112, 236, 109, 1288, 211, 561, 107, 285, 243, 126, 140, 356, 112, 179, 109, 6172, 1471, 511, 268, 155, 6001, 2101, 112, 1309, 164, 109, 366, 107, 198, 187, 140, 708, 784, 114, 3367, 141, 109, 713, 113, 1303, 120, 148, 174, 479, 745, 178, 243, 107, 198, 17570, 126, 117, 773, 121, 13802, 118, 200, 170, 133, 174, 3354, 165, 113, 153, 1463, 111, 109, 979, 124, 1098, 496, 2, 2, 8132, 214, 160, 128, 306, 113, 109, 1288, 111, 199, 126, 140, 5760, 107, 6326, 214, 124, 110, 14222, 35161, 107, 14551, 1384, 65272, 107, 1572, 107, 2723, 132, 110, 36532, 77899, 1384, 65272, 107, 1572, 107, 2723, 107, 1], [202, 1316, 5929, 687, 299, 134, 109, 6625, 6453, 115, 4093, 1411, 134, 160, 79846, 3142, 54493, 124, 1327, 111, 1621, 195, 1049, 112, 858, 109, 1221, 107, 398, 157, 5232, 833, 157, 1148, 109, 228, 7390, 108, 13762, 477, 121, 1846, 121, 3575, 115, 109, 439, 1669, 108, 46215, 141, 17329, 107, 614, 113, 109, 1473, 1211, 117, 135, 2579, 108, 109, 176, 135, 1224, 111, 9697, 107, 168, 140, 153, 211, 565, 115, 3701, 3360, 107, 139, 1712, 113, 156, 113, 109, 7390, 243, 223, 113, 109, 4828, 196, 518, 510, 12074, 124, 1042, 111, 219, 196, 174, 6487, 107, 2595, 1211, 133, 7219, 2408, 5646, 111, 138, 1213, 153, 1473, 113, 109, 2523, 3682, 678, 197, 157, 196, 2771, 107, 3385, 133, 19440, 118, 257, 160, 109, 2281, 107, 2, 198, 1641, 109, 3015, 1007, 117, 309, 365, 3244, 108, 126, 117, 666, 120, 109, 1316, 140, 547, 14981, 496, 1], [15458, 2893, 115, 114, 975, 112, 1459, 430, 109, 976, 19935, 108, 173, 109, 8743, 13261, 153, 3745, 112, 275, 751, 114, 453, 786, 113, 109, 1108, 1732, 107, 17585, 63759, 138, 388, 776, 1573, 113, 320, 121, 12210, 5377, 457, 94462, 107, 139, 278, 6232, 8524, 15680, 11670, 118, 28235, 115, 109, 6887, 7800, 108, 162, 256, 133, 684, 342, 16113, 113, 8622, 107, 343, 37754, 209, 7455, 7734, 114, 75136, 108, 244, 11128, 513, 109, 42287, 243, 198, 1804, 786, 4963, 140, 634, 124, 241, 178, 246, 1669, 2302, 14525, 21173, 59019, 4247, 10843, 2979, 165, 121, 25033, 26573, 320, 121, 12210, 79753, 16185, 124, 169, 11999, 305, 4042, 107, 2, 8743, 195, 19881, 113, 15458, 131, 116, 3644, 269, 9473, 244, 63759, 111, 94462, 1554, 156, 121, 8996, 115, 976, 846, 108, 111, 153, 2084, 2893, 112, 129, 210, 3271, 130, 109, 1108, 1732, 3044, 126, 122, 109, 2691, 224, 205, 113, 9473, 107, 2, 2, 2, 139, 4215, 4079, 8743, 127, 13215, 118, 109, 1580, 108, 254, 175, 15458, 137, 129, 1214, 112, 2589, 183, 107, 63759, 243, 151, 198, 3608, 232, 145, 195, 221, 806, 115, 109, 1580, 111, 125, 311, 145, 127, 115, 234, 1674, 118, 3469, 107, 184, 138, 508, 112, 361, 183, 114, 514, 166, 496, 4247, 10843, 2979, 131, 116, 15179, 118, 169, 4168, 48584, 4042, 195, 571, 135, 1387, 233, 178, 209, 374, 165, 178, 140, 4446, 124, 1789, 173, 42287, 3880, 6130, 24392, 43987, 46170, 262, 113, 114, 2738, 15614, 8123, 115, 169, 1124, 4897, 134, 109, 211, 1580, 113, 109, 578, 115, 1492, 228, 899, 754, 107, 139, 14525, 15679, 196, 112, 3141, 4794, 135, 2466, 108, 241, 178, 196, 174, 1776, 115, 109, 2422, 11999, 439, 178, 5915, 186, 108, 111, 2783, 115, 20168, 209, 539, 269, 211, 846, 124, 1197, 107, 285, 163, 196, 114, 1011, 976, 846, 108, 2362, 149, 155, 109, 976, 2349, 113, 109, 1617, 262, 113, 114, 336, 8186, 107, 16185, 140, 8507, 115, 109, 211, 9473, 1617, 108, 155, 4247, 10843, 2979, 16356, 27593, 342, 141, 68308, 914, 15880, 116, 173, 126, 40145, 107, 139, 7796, 1019, 121, 1623, 243, 151, 198, 187, 1606, 244, 3134, 125, 196, 708, 984, 3644, 112, 79753, 111, 125, 1606, 175, 125, 2521, 114, 332, 588, 125, 256, 1556, 1459, 342, 111, 254, 165, 121, 93534, 342, 111, 120, 117, 180, 148, 2032, 107, 198, 1997, 49783, 117, 114, 221, 234, 11779, 118, 213, 262, 178, 117, 114, 278, 6232, 111, 178, 117, 210, 606, 112, 109, 320, 167, 125, 346, 221, 4032, 122, 109, 9473, 496, 2, 2706, 522, 178, 368, 114, 234, 494, 111, 125, 595, 131, 144, 107, 3695, 108, 221, 234, 494, 107, 184, 1606, 199, 1235, 178, 140, 496, 139, 8548, 177, 14040, 9473, 327, 140, 11079, 118, 136, 1580, 2409, 1518, 6228, 134, 109, 211, 1580, 115, 1492, 112, 275, 247, 112, 109, 1680, 327, 107, 42287, 1977, 7026, 413, 252, 144, 243, 1678, 124, 1327, 120, 178, 198, 37444, 126, 993, 112, 361, 177, 9473, 156, 154, 1012, 194, 108, 1779, 151, 198, 284, 498, 115, 114, 278, 241, 186, 117, 314, 249, 204, 4658, 496, 139, 327, 947, 124, 109, 1444, 113, 5873, 164, 109, 5826, 114, 332, 233, 4536, 1144, 131, 116, 31885, 23053, 2371, 164, 165, 113, 975, 115, 1204, 307, 295, 244, 109, 320, 7994, 80217, 109, 6002, 113, 169, 976, 550, 108, 2096, 342, 146, 505, 166, 112, 573, 126, 269, 109, 14040, 4500, 26316, 342, 165, 107, 343, 126, 138, 331, 115, 118, 154, 7881, 130, 114, 711, 113, 1905, 113, 1103, 918, 134, 109, 370, 113, 276, 1617, 107, 2, 2786, 156, 439, 233, 24679, 32391, 12824, 131, 116, 4536, 1144, 233, 140, 165, 124, 109, 1103, 122, 1029, 542, 112, 275, 107, 2, 139, 1518, 127, 988, 122, 413, 252, 144, 111, 1091, 740, 1162, 5388, 22560, 62795, 11074, 124, 1342, 134, 9105, 391, 166, 112, 1854, 124, 180, 112, 171, 122, 9473, 118, 109, 1004, 113, 109, 578, 107, 413, 252, 144, 243, 178, 140, 198, 27453, 5678, 10855, 194, 157, 192, 129, 350, 112, 1111, 26722, 2158, 124, 114, 411, 107, 198, 284, 246, 2427, 112, 109, 200, 2190, 124, 1180, 745, 88652, 243, 107, 198, 359, 157, 127, 309, 13197, 108, 162, 125, 346, 334, 157, 138, 129, 108, 145, 246, 411, 126, 496, 1714, 10220, 131, 116, 4767, 81300, 140, 4575, 124, 109, 5826, 108, 1573, 113, 109, 4225, 1732, 113, 1328, 95129, 87226, 111, 42889, 59525, 111, 4536, 1144, 131, 116, 24679, 32391, 12824, 107, 81300, 131, 116, 320, 121, 12210, 32390, 4935, 1046, 15434, 2130, 140, 10594, 333, 109, 453, 1617, 233, 230, 487, 109, 320, 131, 116, 10779, 233, 111, 109, 25504, 113, 25360, 67774, 21011, 15943, 209, 2079, 1925, 307, 5266, 107, 2546, 8743, 88321, 35853, 184, 8036, 32879, 2079, 142, 994, 1195, 307, 115, 109, 13690, 439, 107, 20168, 9688, 9473, 602, 20168, 9688, 1771, 703, 1], [1084, 7535, 25458, 108, 10043, 113, 61371, 108, 35728, 108, 155, 239, 622, 115, 1169, 108, 4121, 114, 916, 113, 2168, 2699, 108, 330, 228, 7751, 113, 115, 2534, 70228, 122, 114, 667, 107, 139, 9967, 121, 1019, 121, 1623, 117, 6462, 113, 14757, 109, 26586, 317, 1051, 16390, 111, 1350, 24431, 1263, 25458, 29525, 149, 109, 2699, 107, 2, 198, 159, 10355, 649, 1015, 113, 120, 1851, 2032, 317, 1847, 111, 149, 219, 1273, 107, 285, 649, 157, 127, 149, 64741, 153, 2452, 111, 3403, 4269, 745, 243, 5390, 22321, 107, 139, 19967, 4620, 1263, 25458, 3274, 156, 738, 232, 459, 112, 169, 238, 1192, 342, 109, 1012, 112, 286, 134, 63621, 3265, 266, 134, 27248, 9245, 155, 237, 2375, 342, 86412, 3265, 107, 452, 898, 109, 8376, 120, 109, 2955, 140, 237, 23137, 16238, 2096, 342, 6436, 111, 28058, 107, 5390, 22321, 243, 151, 198, 159, 38262, 131, 116, 49655, 117, 120, 124, 114, 344, 113, 5430, 5547, 4091, 192, 1391, 122, 109, 10355, 707, 115, 109, 10355, 131, 116, 439, 132, 115, 169, 8050, 496, 452, 898, 109, 8376, 114, 453, 2955, 140, 784, 141, 1263, 25458, 118, 114, 1339, 115, 1169, 134, 109, 779, 113, 1428, 132, 1265, 111, 244, 2219, 17273, 178, 140, 678, 23137, 16238, 107, 5390, 22321, 243, 228, 3185, 135, 109, 61371, 456, 196, 163, 266, 6337, 113, 270, 23137, 16238, 107, 2, 139, 2498, 108, 162, 117, 1214, 112, 289, 228, 899, 108, 2138, 107, 1], [12432, 111, 648, 195, 30417, 135, 55852, 15303, 21458, 2241, 124, 1900, 244, 114, 729, 2886, 791, 134, 109, 5602, 8926, 112, 3832, 1847, 111, 536, 107, 15181, 195, 8736, 112, 9202, 122, 109, 729, 108, 114, 758, 1564, 3319, 107, 14740, 1574, 120, 109, 7355, 729, 196, 784, 500, 200, 30926, 5343, 9206, 107, 139, 3522, 5705, 113, 55852, 15303, 21458, 2241, 108, 14245, 35263, 457, 54177, 9801, 16437, 108, 170, 140, 30417, 135, 109, 1944, 108, 243, 120, 186, 196, 174, 198, 1804, 30926, 21508, 194, 108, 1779, 120, 109, 729, 140, 198, 15557, 115, 109, 418, 2302, 2, 285, 243, 120, 109, 2241, 196, 2255, 3514, 114, 731, 10431, 120, 109, 729, 246, 146, 129, 6413, 112, 1635, 114, 3785, 107, 198, 5277, 19304, 140, 784, 429, 745, 982, 54177, 9801, 16437, 243, 108, 1779, 120, 109, 3785, 115, 109, 3319, 131, 116, 6648, 124, 1900, 140, 146, 169, 3020, 19304, 107, 139, 4077, 472, 10754, 5668, 115, 17035, 645, 500, 3613, 115, 9702, 633, 108, 330, 109, 10011, 8171, 124, 109, 79610, 28827, 124, 351, 2024, 131, 116, 8197, 162, 518, 6341, 200, 2609, 107, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaf5080",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7afbd346",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(text_path, 'r', encoding='utf-8') as fp:\n",
    "    train_texts = fp.read().split('\\n')\n",
    "\n",
    "with open(label_path, 'r', encoding='utf-8') as fp:\n",
    "    train_labels = fp.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06f42c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The wooden building is at Abersoch on the Llŷn Peninsula in Gwynedd. Measuring just 13ft by 9ft, it has no electricity or water - and you are banned from sleeping in it overnight. For the same price just a few miles away you could snap-up a two-bedroom house in the village of Llanbedrog - or even a seven-bedroom terraced house at Tywyn across Cardigan Bay. <mask_1> \"It\\'s quite incredible. We had two very determined bidders, both from the Cheshire area, who were bidding separately. They were very determined to buy it.\" The auctioneers said the hut is \"in need of some TLC\" - but does include part of the beach in front of the hut into the sea. The previous record for the beach huts on the Abersoch sands was £70,000 in 2008.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6386b072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"It\\'s certainly the highest price ever achieved for a beach hut in Abersoch,\" remarked Tony Webber, auction surveyor at Beresford Adams Countrywide Auctions. '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_texts[106]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a66fd5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PegasusDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels['input_ids'][idx])\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fcd133f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(train_texts, return_tensors='pt', padding=True, truncation=True)\n",
    "labels = tokenizer(train_labels, return_tensors='pt', padding=True, truncation=True)\n",
    "dataset = PegasusDataset(inputs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ddf252",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008307fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,           \n",
    "    num_train_epochs=30,          \n",
    "    per_device_train_batch_size=4,               \n",
    "    save_total_limit=5,                           \n",
    "    weight_decay=0.01,               \n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                        \n",
    "    args=training_args,                  \n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
