{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db22c49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DuongNgoKien\\AppData\\Local\\anaconda3\\envs\\api\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer, Trainer, TrainingArguments\n",
    "import evaluate\n",
    "import torch\n",
    "import evaluate\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cb145c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "output_dir = './results'\n",
    "model_name = 'google/pegasus-large'\n",
    "text_path = 'texts.txt'\n",
    "label_path = 'labels.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1774a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.encoder.embed_positions.weight', 'model.decoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7afbd346",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(text_path, 'r', encoding='utf-8') as fp:\n",
    "    train_texts = fp.read().split('\\n')\n",
    "\n",
    "with open(label_path, 'r', encoding='utf-8') as fp:\n",
    "    train_labels = fp.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06f42c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The wooden building is at Abersoch on the Llŷn Peninsula in Gwynedd. Measuring just 13ft by 9ft, it has no electricity or water - and you are banned from sleeping in it overnight. For the same price just a few miles away you could snap-up a two-bedroom house in the village of Llanbedrog - or even a seven-bedroom terraced house at Tywyn across Cardigan Bay. <mask_1> \"It\\'s quite incredible. We had two very determined bidders, both from the Cheshire area, who were bidding separately. They were very determined to buy it.\" The auctioneers said the hut is \"in need of some TLC\" - but does include part of the beach in front of the hut into the sea. The previous record for the beach huts on the Abersoch sands was £70,000 in 2008.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts[106]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6386b072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"It\\'s certainly the highest price ever achieved for a beach hut in Abersoch,\" remarked Tony Webber, auction surveyor at Beresford Adams Countrywide Auctions. '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_texts[106]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a66fd5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PegasusDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels['input_ids'][idx])\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fcd133f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(train_texts, return_tensors='pt', padding=True, truncation=True)\n",
    "labels = tokenizer(train_labels, return_tensors='pt', padding=True, truncation=True)\n",
    "dataset = PegasusDataset(inputs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ddf252",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008307fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,           \n",
    "    num_train_epochs=30,          \n",
    "    per_device_train_batch_size=4,               \n",
    "    save_total_limit=5,                           \n",
    "    weight_decay=0.01,               \n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                        \n",
    "    args=training_args,                  \n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
